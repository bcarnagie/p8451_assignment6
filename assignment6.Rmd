---
title: "Assignment 6: Comparison between Classification Trees, SVM and Logistic Regression"
date: "2024-02-26"
output: word_document
---

```{r load_packages, message=FALSE}
library(tidyverse)
library(rpart)
library(caret)
library(rpart.plot)
library(pROC)
library(kernlab)
library(e1071)

```

This analysis follows the methodology presented by Yu et al., which utilizes NHANES data from 1999-2004 to predict diabetes and pre-diabetes using Support Vector Machines (SVM). Our goal is to predict Diabetes using a similar set of variables but acknowledging that the available data and outcomes may differ, leading to unique insights.

### Question 1 & 2: Data Loading and Preprocessing

```{r}
set.seed(123)
library(NHANES)

nhanes = NHANES %>% janitor::clean_names()

nhanes = nhanes |> select(age, race1, education, hh_income, weight, height, pulse, diabetes, bmi, phys_active, smoke100) |>  na.omit()

partition <- createDataPartition(y = nhanes$diabetes, p = 0.7, list = FALSE)

# Creating training and testing sets
train_data <- nhanes[partition, ]
test_data <- nhanes[-partition, ]
```


## Question 3: Model Training and Evaluation

### Classification Tree
```{r}
set.seed(123)
# Set up 10-fold cross-validation with down-sampling for imbalance 
train.control <- trainControl(method = "cv", number = 10, sampling = "down", summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = TRUE)

# Create a sequence of cp values to try
cpGrid <- expand.grid(cp = seq(0.001, 0.3, by = 0.01))

# Train the model
tree.diabetes <- train(
  diabetes ~ .,
  data = train_data,
  method = "rpart",
  trControl = train.control,
  tuneGrid = cpGrid,
  metric = "ROC"
)

# Best tuning parameter
tree.diabetes$bestTune

# Plotting the tree
rpart.plot(tree.diabetes$finalModel)

# Variable importance
varImp(tree.diabetes)

# Printing Model 
print(tree.diabetes)
```
AUC for the Classification Tree model is 0.7891.

### Support Vector Classifier (i.e. Support Vector Machine with a linear classifier)
```{r, message=FALSE}
set.seed(123)

# Setting up cross-validation and specifying AUC as the metric
trainControl <- trainControl(method = "cv",
                             number = 10,
                             summaryFunction = twoClassSummary,
                             classProbs = TRUE, 
                             savePredictions = "final")

# Define a tuning grid for SVM hyperparameters, focusing on 'C'
tuningGrid <- expand.grid(C = seq(0.001, 2, length = 20))

# Train the SVM model with cross-validation
svmModel <- train(diabetes ~ .,
                  data = train_data,
                  method = "svmLinear",
                  trControl = trainControl,
                  tuneGrid = tuningGrid,
                  metric = "ROC",
                  preProcess = c("center", "scale")) # Pre-processing steps

# Print Model
print(svmModel)
```
AUC for the SVM model is 0.6378.

### Logistic regression
```{r}
set.seed(123)

# Set up cross-validation with down-sampling for imbalance handling
train.control <- trainControl(method = "cv", number = 10, summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = TRUE, sampling = "down")

# Train the logistic regression model
logistic.diabetes <- train(
  diabetes ~ .,
  data = train_data,
  method = "glm",
  family = "binomial",
  trControl = train.control,
  metric = "ROC"
)

# Display the model
print(logistic.diabetes)


# model coefficients as an indication of importance
coef(logistic.diabetes$finalModel)
```
AUC for the logistic regression model is 0.8117.

## Question 4: Optimal Model Selection

The optimal model that I have chosen is the *logistic regression* due to its higher AUC value, indicating it has the best overall ability to distinguish between the classes across all thresholds. It also has slightly higher sensitivity, meaning it's marginally better at identifying true positives and slightly higher specificity, indicating a marginally better performance at identifying true negatives.

```{r}
# Prediction and Evaluation on the Testing Set

# Create predictions on the test set
pred.diabetes <- predict(logistic.diabetes, newdata = test_data)

# Evaluation results on the test set
eval.results <- confusionMatrix(pred.diabetes, test_data$diabetes, positive = "Yes")
print(eval.results)

# Predictions as probabilities on the test set
pred.diabetes.prob <- predict(logistic.diabetes, newdata = test_data, type = "prob")

# ROC analysis
roc.analysis.2 <- roc(response = as.numeric(test_data$diabetes), predictor = pred.diabetes.prob[,2])
plot(roc.analysis.2, print.auc = TRUE, main = "ROC Curve for Diabetes Prediction")
```

**Model Performance Overview**
* The model demonstrates fair ability to identify positive cases but struggles with a high false positive rate, as indicated by the low PPV.
* The high NPV and sensitivity suggest the model is quite conservative, effectively identifying negative cases but at the cost of missing or incorrectly predicting a significant portion of positive cases.


## Question 5: Ethical Considerations of Using Race in Predictive Modeling

Including race in disease prediction models is a complex issue with important ethical considerations. On one side, using race can help address health inequalities by recognizing that different racial groups often face different health challenges due to factors like socioeconomic status and access to healthcare. This means that including race could make predictions more accurate for everyone, potentially leading to better health care for underserved groups.

However, there are significant concerns about the negative effects of including race. It can reinforce harmful stereotypes and suggest that there are biological differences between races, which is not accurate. This could lead to unfair treatment in healthcare, where people of different races receive different levels of care even if they have the same health conditions. There's also a worry that using race in this way could make health disparities worse, not better. While including race in prediction models might aim to make healthcare more fair and accurate, it's crucial to ensure that it doesn't accidentally cause more harm than good. The challenge is to use this information wisely to improve health outcomes for everyone without reinforcing biases or inequalities.

